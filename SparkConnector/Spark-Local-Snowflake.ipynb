{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Jupyter Notebook Integration with Snowflake via local Spark\n",
    "\n",
    "---\n",
    "This Notebook shows how to integrate Sagemaker and Snowflake so you can store data in Snowflake and import it into a Jupyter Notebook via the [Spark connector](https://docs.snowflake.net/manuals/user-guide/spark-connector.html). In this particular case, Spark is running on the Notebook server as a local instance. The Jupyter Notebook runs a python kernel . To connect to Snowflake we need to install the Snowflake Connector locally on the Notebook server.\n",
    "\n",
    "Even though the spark is running on the local Notebook server, it's already much more scalalbe than the Python-connector solution. In this example we will import 5 million rows. However, keep in mind that processing of bigger datasets on a single machine takes its time. Open a termial session from the Jupyter UI in case you like to see what's going on on the Notebook server. \n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Driver Installation](#Driver-Installation)\n",
    "1. [Spark Context](#Spark-Context)\n",
    "1. [Credentials](#Credentials)\n",
    "1. [Data Import](#Data-Import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Installation\n",
    "The following step installs the latest version of both drivers needed, i.e. JDBC and Spark. If the latest version of the jar files doesn't exist on the Notebook server, all previous jars will be deleted and the newest version will be downloaded from a [maven repository](https://repo1.maven.org/maven2/net/snowflake/). Please note that the the Snowflake driver jars will be installed in their own directory under `/home/ec2-user/snowflake`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "SFC_DIR=/home/ec2-user/snowflake\n",
    "[ ! -d \"$SFC_DIR\" ] && mkdir $SFC_DIR \n",
    "cd $SFC_DIR\n",
    "PRODUCTS='snowflake-jdbc spark-snowflake_2.11'\n",
    "for PRODUCT in $PRODUCTS\n",
    "do\n",
    "   wget \"https://repo1.maven.org/maven2/net/snowflake/$PRODUCT/maven-metadata.xml\" 2> /dev/null\n",
    "   VERSION=$(grep latest maven-metadata.xml | awk -F\">\" '{ print $2 }' | awk -F\"<\" '{ print $1 }')\n",
    "   DRIVER=$PRODUCT-$VERSION.jar\n",
    "   if [[ ! -e $DRIVER ]]\n",
    "   then\n",
    "      rm $PRODUCT* 2>/dev/null\n",
    "      wget \"https://repo1.maven.org/maven2/net/snowflake/$PRODUCT/$VERSION/$DRIVER\" 2> /dev/null\n",
    "   fi\n",
    "   [ -e maven-metadata.xml ] && rm maven-metadata.xml\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "The following step configures the spark context. Please note that the inline shell command enumerates the content of the Snowflake jars directory (, which then will be added to the Spark configuration. Please note that you have to stop the spark context in case you want to re-run the notebook. One way to accomplish that is to restart the kernel, or you can just un-comment the line below and execute it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.152.127:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local-spark-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=local-spark-test>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from sagemaker_pyspark import IAMRole, classpath_jars\n",
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator\n",
    "\n",
    "sfc_jars=!ls -d /home/ec2-user/snowflake/*.jar\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", (\":\".join(classpath_jars())+\":\"+\":\".join(sfc_jars)))\n",
    "        .setMaster('local')\n",
    "        .setAppName('local-spark-test'))\n",
    "sc=SparkContext(conf=conf)\n",
    "\n",
    "spark = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "Credentials can be hard coded but a much more secure way is to stored them in the [Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html). The following step reads the values for the provided keys from the parameter store. These Keys are just an example. You can use the same Keys but you have to create the Key/Value pairs in the parameter store before you can use them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "params=['/SNOWFLAKE/URL','/SNOWFLAKE/ACCOUNT_ID'\n",
    "        ,'/SNOWFLAKE/USER_ID','/SNOWFLAKE/PASSWORD'\n",
    "        ,'/SNOWFLAKE/DATABASE','/SNOWFLAKE/SCHEMA'\n",
    "        ,'/SNOWFLAKE/WAREHOUSE','/SNOWFLAKE/BUCKET'\n",
    "        ,'/SNOWFLAKE/PREFIX']\n",
    "\n",
    "region='us-east-1'\n",
    "\n",
    "def get_credentials(params):\n",
    "   ssm = boto3.client('ssm',region)\n",
    "   response = ssm.get_parameters(\n",
    "      Names=params,\n",
    "      WithDecryption=True\n",
    "   )\n",
    "   #Build dict of credentials\n",
    "   param_values={k['Name']:k['Value'] for k in  response['Parameters']}\n",
    "   return param_values\n",
    "\n",
    "param_values=get_credentials(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "The following step reads weather from the [Snowflake Sample Weather Data](https://docs.snowflake.net/manuals/user-guide/sample-data-openweathermap.html) database. Notice, how easy it is to read and transform JSON data. The result set can directly be used to create a pandas data frame. Check out this [JSON tutorial](https://docs.snowflake.net/manuals/user-guide/json-basics-tutorial.html) on the Snowflake documentation site.\n",
    "\n",
    "Credentials for reading data from Snowflake are passed via an options Array. The specific values have been read in the step above. To ensure that the Notebook server doesn't crash we import only 5 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|      TEMP_MAX_FAR|      TEMP_MIN_FAR|               LAT|               LON|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|           5000000|           5000000|           5000000|           5000000|\n",
      "|   mean|  70.4678645559177|  68.1955592330811| 28.01917999733069|14.799504624313634|\n",
      "| stddev|15.307197922767305|15.218515366656854|22.735369707500286|  70.9452732743914|\n",
      "|    min|          -42.5596|          -42.5596|          -0.03333|          -0.00421|\n",
      "|    max|             131.9|             131.0|           9.99559|         99.993423|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : param_values['/SNOWFLAKE/URL'],\n",
    "  \"sfAccount\" : param_values['/SNOWFLAKE/ACCOUNT_ID'],\n",
    "  \"sfUser\" : param_values['/SNOWFLAKE/USER_ID'],\n",
    "  \"sfPassword\" : param_values['/SNOWFLAKE/PASSWORD'],\n",
    "  \"sfDatabase\" : param_values['/SNOWFLAKE/DATABASE'],\n",
    "  \"sfSchema\" : param_values['/SNOWFLAKE/SCHEMA'],\n",
    "  \"sfWarehouse\" : param_values['/SNOWFLAKE/WAREHOUSE'],\n",
    "}\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "  .options(**sfOptions) \\\n",
    "  .option(\"query\", \\\n",
    "\"select (V:main.temp_max - 273.15) * 1.8000 + 32.00 as temp_max_far, \" +\\\n",
    "\"       (V:main.temp_min - 273.15) * 1.8000 + 32.00 as temp_min_far, \" +\\\n",
    "\"       cast(V:time as timestamp) time, \" +\\\n",
    "\"       V:city.coord.lat lat, \" +\\\n",
    "\"       V:city.coord.lon lon \" +\\\n",
    "\"from snowflake_sample_data.weather.weather_14_total limit 5000000\").load()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
