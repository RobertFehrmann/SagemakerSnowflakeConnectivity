{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Jupyter Notebook Integration with Snowflake via EMR Spark\n",
    "\n",
    "---\n",
    "This Notebook shows how to integrate Sagemaker and Snowflake so you can store data in Snowflake and import it into a Jupyter Notebook via the [Spark connector](https://docs.snowflake.net/manuals/user-guide/spark-connector.html). In this particular case, Spark is running on an EMR cluster. The Jupyter Notebook runs a PySpark kernel . To connect to snowflake we need to install the Snowflake Spark Connector on the EMR cluster. \n",
    "\n",
    "The easiest way to get started is to configure the Sagemaker environment and the EMR environment in the same VPC.\n",
    "\n",
    "Building an EMR cluster that can be used with Sagemaker requires a couple of customizations. At a minimum you will need to make changes in the following sections\n",
    "\n",
    "1. Advanced Options\n",
    " * Use the Advanced options link to configure all necessary options\n",
    "1. Software and Steps\n",
    " * Pick Hadoop, and Spark \n",
    " * Optionally Zeppelin and Ganglia\n",
    "1. Hardware\n",
    " * Validate the VPC (Network). The Sagemaker host needs to be created in the same VPC. \n",
    " * Optionally you can change the instance types and whether or not to use spot pricing.\n",
    "1. General Cluster Settings\n",
    " * Set the Cluster name\n",
    " * Keep Logging to troubleshoot problems\n",
    " * Pick the Bootstrap Action. [see below](#Bootstrap)\n",
    "1. Security\n",
    " * Pick an EC2 key pair (create you don't have one yet). Without the key pair you won't be able to access the master node via ssh to finalize the setup\n",
    " * Create and additional security group to enable access via SSH and Livy [see below](#Additional-Security-Groups) \n",
    " \n",
    " \n",
    "## Bootstrap\n",
    "\n",
    "Sagemaker and Snowflake require additional JAR files to be deployed to both, the EMR master and EMR worker nodes. The easiest way to deploy these libraries is via a bootstrap script. The bootstrap script must be stored in an S3 bucket\n",
    "\n",
    "## Additional Security Groups\n",
    "\n",
    "Sagemaker is using the livy API (hosted on the EMR cluster) to access Spark resources. By default, access to the livy API is disable. Best practice is to create a new security group assigned to the EMR cluster which allows inbound traffic on port 8998 coming from the security group assigned to the Sagemaker host. For this to work, the Sagemaker host has to run in a VPC. \n",
    "\n",
    "The second reason for creating a security group is to enable SSH. SSH is needed at least for the EMR master node since we have to modify the spark-defaults configuration file.  creation.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Notebook Configuration](#Notebook-Configuration)\n",
    "1. [Credentials](#Credentials)\n",
    "1. [Data Import](#Data-Import)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "The default configuration for a pyspark Notebook is to use localhost as the Spark environment. You have to change the sparkmagic configuration for the Sagemaker Kernel to point to the EMR master node instead of `localhost`. Update the environment variable \n",
    "\n",
    "`EMR_MASTER_INTERNAL_IP` \n",
    "\n",
    "with the internal IP of the EMR master node. Be sure to provide the internal IP and not the public IP. \n",
    "Unfortunately, Sagemaker does not dynamically update it's runtime parameters when the config file has been updated, you need to restart the Sagemaker Kernel via \n",
    "\n",
    "`Kernel->Restart` \n",
    "\n",
    "Generally speaking teh config file needs to be updated upon the initial run as well as whenever the IP of the EMR master node has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "EMR_MASTER_INTERNAL_IP=ip-172-31-58-190.ec2.internal\n",
    "CONF=/home/ec2-user/.sparkmagic/config.json\n",
    "if [[ ! -e $CONF.bk ]]\n",
    "then\n",
    "   wget \"https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json\" \\\n",
    "-P /home/ec2-user/.sparkmagic -O /home/ec2-user/.sparkmagic/config.json.bk 2>/dev/null\n",
    "fi\n",
    "cat $CONF.bk | sed \"s/localhost/$EMR_MASTER_INTERNAL_IP/\" > $CONF.new\n",
    "if [[ $(diff $CONF.new $CONF) ]]\n",
    "then\n",
    "   echo \"Configuration has changed; Restart Kernel\"\n",
    "fi\n",
    "cp $CONF.new $CONF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "Credentials can be hard coded but a much more secure way is to stored them in the [Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html). The following step reads the values for the provided keys from the parameter store. These Keys are just an example. You can use the same Keys but you have to create the Key/Value pairs in the parameter store before you can use them here. \n",
    "\n",
    "The PySpark kernel automatically creates a spark context upon running the first step on the Spark cluster. Therefore it creates the spark context when we are running the boto3 script below the get the database credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "params=['/SNOWFLAKE/URL','/SNOWFLAKE/ACCOUNT_ID'\n",
    "        ,'/SNOWFLAKE/USER_ID','/SNOWFLAKE/PASSWORD'\n",
    "        ,'/SNOWFLAKE/DATABASE','/SNOWFLAKE/SCHEMA'\n",
    "        ,'/SNOWFLAKE/WAREHOUSE','/SNOWFLAKE/BUCKET'\n",
    "        ,'/SNOWFLAKE/PREFIX']\n",
    "\n",
    "region='us-east-1'\n",
    "\n",
    "def get_credentials(params):\n",
    "   ssm = boto3.client('ssm',region)\n",
    "   response = ssm.get_parameters(\n",
    "      Names=params,\n",
    "      WithDecryption=True\n",
    "   )\n",
    "   #Build dict of credentials\n",
    "   param_values={k['Name']:k['Value'] for k in  response['Parameters']}\n",
    "   return param_values\n",
    "\n",
    "param_values=get_credentials(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "The following step reads weather from the [Snowflake Sample Weather Data](https://docs.snowflake.net/manuals/user-guide/sample-data-openweathermap.html) database. Notice, how easy it is to read and transform JSON data. The result set can directly be used to create a pandas data frame. Check out this [JSON tutorial](https://docs.snowflake.net/manuals/user-guide/json-basics-tutorial.html) on the Snowflake documentation site.\n",
    "\n",
    "Since we are running a scalable spark environment, we now can read the whole dataset, i.e. about 220 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|     TEMP_MAX_FAR|      TEMP_MIN_FAR|               LAT|               LON|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|        226196451|         226196451|         226196451|         226196451|\n",
      "|   mean|64.54348749225441| 62.50216523693793| 27.97162737457021|14.959025391770865|\n",
      "| stddev| 18.8954069539488|19.277845772751554|22.682531505927834| 71.01645033334991|\n",
      "|    min|         -62.3974|          -62.3974|          -0.03333|          -0.00421|\n",
      "|    max|            168.8|             138.2|           9.99559|         99.993423|\n",
      "+-------+-----------------+------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : param_values['/SNOWFLAKE/URL'],\n",
    "  \"sfAccount\" : param_values['/SNOWFLAKE/ACCOUNT_ID'],\n",
    "  \"sfUser\" : param_values['/SNOWFLAKE/USER_ID'],\n",
    "  \"sfPassword\" : param_values['/SNOWFLAKE/PASSWORD'],\n",
    "  \"sfDatabase\" : param_values['/SNOWFLAKE/DATABASE'],\n",
    "  \"sfSchema\" : param_values['/SNOWFLAKE/SCHEMA'],\n",
    "  \"sfWarehouse\" : param_values['/SNOWFLAKE/WAREHOUSE'],\n",
    "}\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "  .options(**sfOptions) \\\n",
    "  .option(\"query\", \\\n",
    "\"select (V:main.temp_max - 273.15) * 1.8000 + 32.00 as temp_max_far, \" +\\\n",
    "\"       (V:main.temp_min - 273.15) * 1.8000 + 32.00 as temp_min_far, \" +\\\n",
    "\"       cast(V:time as timestamp) time, \" +\\\n",
    "\"       V:city.coord.lat lat, \" +\\\n",
    "\"       V:city.coord.lon lon \" +\\\n",
    "\"from snowflake_sample_data.weather.weather_14_total\").load()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
